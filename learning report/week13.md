# 常用聚类算法
 
------
 
**目录** 如下：
 
> * 聚类和相似度度量
> * 数据聚类方法
------

### 1.  聚类和相似度度量
> * 1.1聚类的定义
>聚类(Clustering)是按照某个特定标准(如距离)把一个数据集分割成不同的类或簇，使得同一个簇内的数据对象的相似性尽可能大，同时不在同一个簇中的数据对象的差异性也尽可能地大。也即聚类后同一类的数据尽可能聚集到一起，不同类数据尽量分离。
> * 1.2聚类和分类的区别
>聚类(Clustering)：是指把相似的数据划分到一起，具体划分的时候并不关心这一类的标签，目标就是把相似的数据聚合到一起，聚类是一种无监督学习(Unsupervised Learning)方法。
>分类(Classification)：是把不同的数据划分开，其过程是通过训练数据集获得一个分类器，再通过分类器去预测未知数据，分类是一种监督学习(Supervised Learning)方法。
> * 1.3 聚类的一般过程
>数据准备：特征标准化和降维
>特征选择：从最初的特征中选择最有效的特征，并将其存储在向量中
>特征提取：通过对选择的特征进行转换形成新的突出特征
>聚类：基于某种距离函数进行相似度度量，获取簇
>聚类结果评估：分析聚类结果，如距离误差和(SSE)等
> * 1.4 数据对象间的相似度度量
>对于数值型数据，可以使用下表中的相似度度量方法。
>![avatar](\相似度度量方法.jpg)
> * 1.5 cluster之间的相似度度量
除了需要衡量对象之间的距离之外，有些聚类算法（如层次聚类）还需要衡量cluster之间的距离 ，假设Ci和Cj为两个 cluster，则前四种方法定义的Ci和Cj之间的距离如下表所示。
>![avatar](\cluster相似度度量方法.jpg)
> * Single-link定义两个cluster之间的距离为两个cluster之间距离最近的两个点之间的距离，这种方法会在聚类的过程中产生链式效应，即有可能会出现非常大的cluster
> * Complete-link定义的是两个cluster之间的距离为两个cluster之间距离最远的两个点之间的距离，这种方法可以避免链式效应,对异常样本点（不符合数据集的整体分布的噪声点）却非常敏感，容易产生不合理的聚类
> * UPGMA正好是Single-link和Complete-link方法的折中，他定义两个cluster之间的距离为两个cluster之间所有点距离的平均值
> * 最后一种WPGMA方法计算的是两个 cluster 之间两个对象之间的距离的加权平均值，加权的目的是为了使两个 cluster 对距离的计算的影响在同一层次上，而不受 cluster 大小的影响，具体公式和采用的权重方案有关。
---
### 2.数据聚类方法
 > 数据聚类方法主要可以分为划分式聚类方法(Partition-based Methods)、基于密度的聚类方法(Density-based methods)、层次化聚类方法(Hierarchical Methods)等。
 >![avatar](\数据聚类方法.jpg)
 > * 2.1 划分式聚类方法
划分式聚类方法需要事先指定簇类的数目或者聚类中心，通过反复迭代，直至最后达到"簇内的点足够近，簇间的点足够远"的目标。经典的划分式聚类方法有k-means及其变体k-means++、bi-kmeans、kernel k-means等。
 > * 2.1.2 k-means算法
 >![avatar](\k-means.jpg)
 >一般来说，经典k-means算法有以下几个特点：
>1.需要提前确定k值
>2.对初始质心点敏感
>3.对异常数据敏感
> * 2.1.2 k-means++算法
>k-means++是针对k-means中初始质心点选取的优化算法。该算法的流程和k-means类似，改变的地方只有初始质心的选取。
> * 2.1.3 bi-kmeans算法
>一种度量聚类效果的指标是SSE(Sum of Squared Error)，他表示聚类后的簇离该簇的聚类中心的平方和，SSE越小，表示聚类效果越好。 bi-kmeans是针对kmeans算法会陷入局部最优的缺陷进行的改进算法。该算法基于SSE最小化的原理，首先将所有的数据点视为一个簇，然后将该簇一分为二，之后选择其中一个簇继续进行划分，选择哪一个簇进行划分取决于对其划分是否能最大程度的降低SSE的值。
> * 2.2 基于密度的方法
k-means算法对于凸性数据具有良好的效果，能够根据距离来讲数据分为球状类的簇，但对于非凸形状的数据点，就无能为力了，当k-means算法在环形数据的聚类时，我们看看会发生什么情况。
> * 2.2.1 DBSCAN算法
> * 2.2.2 OPTICS算法
> * 2.3 层次化聚类方法
> 前面介绍的几种算法确实可以在较小的复杂度内获取较好的结果，但是这几种算法却存在一个链式效应的现象，比如：A与B相似，B与C相似，那么在聚类的时候便会将A、B、C聚合到一起，但是如果A与C不相似，就会造成聚类误差，严重的时候这个误差可以一直传递下去。为了降低链式效应，这时候层次聚类就该发挥作用了。
> `层次聚类算法` (hierarchical clustering) 将数据集划分为一层一层的 clusters，后面一层生成的 clusters 基于前面一层的结果。层次聚类算法一般分为两类：
> * Agglomerative 层次聚类：又称自底向上（bottom-up）的层次聚类，每一个对象最开始都是一个 cluster，每次按一定的准则将最相近的两个 cluster 合并生成一个新的 cluster，如此往复，直至最终所有的对象都属于一个 cluster。这里主要关注此类算法。
> * Divisive 层次聚类： 又称自顶向下（top-down）的层次聚类，最开始所有的对象均属于一个 cluster，每次按一定的准则将某个 cluster 划分为多个 cluster，如此往复，直至每个对象均是一个 cluster。
> ![avatar](\层次聚类.jpg)
> 另外，需指出的是，层次聚类算法是一种贪心算法（greedy algorithm），因其每一次合并或划分都是基于某种局部最优的选择。
> * 2.3.1 Agglomerative算法
> 给定数据集 X = {x(1), x(2), ..., x(n)}，Agglomerative层次聚类最简单的实现方法分为以下几步：
> ![avatar](\a算法.jpg)
> Agglomerative算法源代码，可以看到，该 算法的时间复杂度为O(n3)（由于每次合并两个 cluster 时都要遍历大小为 O(n2) 的距离矩阵来搜索最小距离，而这样的操作需要进行 n - 1 次），空间复杂度为O(n2) （由于要存储距离矩阵）。
>  ![avatar](\4种度量方法.jpg)
>  上图中分别使用了层次聚类中4个不同的cluster度量方法，可以看到，使用single-link确实会造成一定的链式效应，而使用complete-link则完全不会产生这种现象，使用average-link和ward-link则介于两者之间。
>  * 2.4 聚类方法比较
>  ![avatar](\4种方法比较.jpg)
 ---